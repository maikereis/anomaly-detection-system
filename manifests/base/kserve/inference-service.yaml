apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: anomaly-detector-predictor
  annotations:
    # Desabilita o autoscaler do KServe/Knative
    autoscaling.knative.dev/class: "hpa.autoscaling.knative.dev"
    autoscaling.knative.dev/metric: "cpu"
    # Evita que o KServe controller modifique as replicas
    serving.kserve.io/autoscalerClass: "hpa"
spec:
  predictor:
    model:
      modelFormat:
        name: mlflow
      runtime: mlflow-runtime
      storageUri: "models:/anomaly-detector/production"

      resources:
        requests:
          cpu: "500m"
          memory: "1Gi"
        limits:
          cpu: "2000m"
          memory: "4Gi"

      env:
        - name: MLFLOW_TRACKING_URI
          value: "http://mlflow-server.ml-dev.svc.cluster.local:5000"
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: mlflow-secret
              key: AWS_ACCESS_KEY_ID
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: mlflow-secret
              key: AWS_SECRET_ACCESS_KEY
        - name: MLFLOW_S3_ENDPOINT_URL
          value: "http://minio-mlflow.ml-dev.svc.cluster.local:9000"
        - name: MLFLOW_S3_IGNORE_TLS
          value: "true"
        - name: AWS_DEFAULT_REGION
          value: "us-east-1"

      volumeMounts:
        - name: model-cache
          mountPath: /models-cache

    volumes:
      - name: model-cache
        emptyDir:
          sizeLimit: 5Gi

    # Canary deployment configuration
    canaryTrafficPercent: 0
